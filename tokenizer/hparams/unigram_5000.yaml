# ############################################################################
# Tokenizer: German Tokenizer
# Training:  SWC, MCV and M-AILABS
# Authors:   Ruhr-University Bochum 2021
#            adapted from Abdel Heba 2021
# ############################################################################

# training parameters
token_type: unigram  # ["unigram", "bpe", "char"]
token_output: 5000  # index(blank/eos/bos/unk) = 0
character_coverage: 1.0
csv_read: words
bos_index: 1
eos_index: 2

# configure output folder and logs
output_folder: !ref results/tokenizer_<token_type>_<token_output>/
train_log: !ref <output_folder>/train_log.txt

# data files
data_folder: !PLACEHOLDER # e.g, /path/to/database
train_splits: ["swc-train.json", "mcv-train.json", "mai-train.json"]
dev_splits: ["swc-dev.json", "mcv-dev.json", "mai-dev.json"]
test_splits: ["swc-test.json", "mcv-test.json", "mai-test.json"]

# concat and convert json to csv
train: !ref <output_folder>/train.csv
dev: !ref <output_folder>/dev.csv
test: !ref <output_folder>/test.csv

# configure tokenizer instance
tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
   model_dir: !ref <output_folder>
   vocab_size: !ref <token_output>
   annotation_train: !ref <train>
   annotation_read: !ref <csv_read>
   model_type: !ref <token_type>
   character_coverage: !ref <character_coverage>
   bos_id: !ref <bos_index> # Define bos_id/eos_id if different from blank_id
   eos_id: !ref <eos_index>
   annotation_list_to_check: [!ref <train>, !ref <dev>, !ref <test>]
